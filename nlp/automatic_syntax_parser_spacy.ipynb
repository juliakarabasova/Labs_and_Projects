{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "automatic_syntax_parser_spacy.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliakarabasova/labs_and_projects/blob/main/nlp/automatic_syntax_parser_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHSIVs1W_Xr3"
      },
      "source": [
        "## Установка библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts02W4jP8JXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706f30d9-5d1b-4826-b614-774f00d0b203"
      },
      "source": [
        "!pip install spacy==3.0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (3.7.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (3.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (20.9)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 57.2MB/s \n",
            "\u001b[?25hCollecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 23.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (4.41.1)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (56.1.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (3.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (2.23.0)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 65.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.5) (2.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0.5) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.5) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy==3.0.5) (3.4.1)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 71.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0.5) (1.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.5) (2020.12.5)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=462b2e055ea05a1a554a5f5b032e2b4a6658027631f8c4a4638ddf396f19cac9\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: pydantic, catalogue, srsly, thinc, typer, smart-open, pathy, spacy-legacy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al94tlxA8LMT"
      },
      "source": [
        "import spacy\n",
        "assert spacy.__version__ == '3.0.5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxRDFkIwKRoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7706c71-875c-4041-962c-e090eb2aacef"
      },
      "source": [
        "!pip install spacy-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/c5/a156f9c979cc14f5f41cf2e6ecfc55d1128ac0363930ec7cc6fe4d98b4a2/spacy_transformers-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (3.0.5)\n",
            "Collecting transformers<4.6.0,>=3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 17.1MB/s \n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/86/a6786d24d1d8f3a6cff2c60b55a7e845725a94919cd94d270ea49d82e59b/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 68.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->spacy-transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->spacy-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (0.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (56.1.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (0.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (3.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (8.0.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (2.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy-transformers) (0.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->spacy-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->spacy-transformers) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->spacy-transformers) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->spacy-transformers) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->spacy-transformers) (1.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->spacy-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->spacy-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->spacy-transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->spacy-transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers) (1.0.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, spacy-alignments, spacy-transformers\n",
            "Successfully installed sacremoses-0.0.45 spacy-alignments-0.8.3 spacy-transformers-1.0.2 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcMbRHAq8ikT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116e67ef-55af-47e5-c030-3def0e80df70"
      },
      "source": [
        "!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-09 15:20:20.334317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-trf==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl (459.7MB)\n",
            "\u001b[K     |████████████████████████████████| 459.7MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy-transformers<1.1.0,>=1.0.0rc4 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (1.0.2)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: transformers<4.6.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (4.5.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.8.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (20.9)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.0.1)\n",
            "Installing collected packages: en-core-web-trf\n",
            "Successfully installed en-core-web-trf-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "2021-05-09 15:21:01.962357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 334kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (56.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i7_CZi6LB2J"
      },
      "source": [
        "**In case of error executing cell below**  \n",
        "\n",
        "Select (top menu item) Runtime->Restart Runtime or **_Ctrl+M_+.**  \n",
        "Then: execute that cell _below_ (upper ones not needed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJcyTIiOKTqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccd717d-05e1-41fe-ff06-dc294a35f437"
      },
      "source": [
        "import spacy # in case of restarting\n",
        "spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7fcbea45b610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj48eDIhURv1"
      },
      "source": [
        "import pandas as pd \n",
        "from spacy.matcher import DependencyMatcher\n",
        "from spacy.matcher import Matcher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQj3If2O9O_P"
      },
      "source": [
        "##SpaCy Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8KrYwKjALXs"
      },
      "source": [
        "\n",
        "Для обработки текста необходимо сначала загрузить pretrained pipeline - это языковая модель класса Language, содержащая vocabulary, language data (правила, исп. для токенизации, лемматизации, извлечения лексических атрибутов и т.д..) и результаты обучения модели (веса).\n",
        "\n",
        "Языковые модели в рамках одного языка отличаются объемом vocabulary и точностью работы компонентов (pipes). Pipes: transformer, tagger, parser, ner, attribute_ruler, lemmatizer, senter, tok2vec, + trainable pipes (non-default).  \n",
        "Список доступных компонентов и анализ результатов работы компонентов можно посмотреть с п. команды ```nlp.analyze_pipes(pretty=True)```, список компонентов: ```nlp.component_names```, где ```nlp = spacy.load('en_core_web_trf')``` или ```nlp = spacy.load('en_core_web_sm')```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJSJ618iZfjJ"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GlU0bYU2p0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a698a48a-b838-4302-a4e3-6c8658766124"
      },
      "source": [
        "nlp.component_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'parser',\n",
              " 'senter',\n",
              " 'ner',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEIs1EJK2ytH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987aac3a-c7c1-42f9-dce1-2a9c643a8001"
      },
      "source": [
        "spacy.load('en_core_web_sm').component_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'parser',\n",
              " 'senter',\n",
              " 'ner',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5_kn-jYGz_G"
      },
      "source": [
        "Параметры точности разных языковых моделей (en) можно посмотреть [здесь](https://spacy.io/models/en) (Accuracy Evaluation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mntgw7BnIsTt"
      },
      "source": [
        "###Processing text\n",
        "\n",
        "Для обработки текста необходимо передать текст в pipeline, результатом будет экземпляр класса [Doc](https://spacy.io/api/doc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rx_efr38z8j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "63881147-96e0-42dc-d3af-3404e3f1239f"
      },
      "source": [
        "# Загрузка модели (nlp = spacy.load('en_core_web_trf'))\n",
        "# Любой текст\n",
        "text = 'The Leonardo da Vinci exhibition is held under the high patronage of French President Emmanuel Macron. \\\n",
        "The year 2019 marks the 500-year anniversary of the death of Leonardo da Vinci in France, of particular importance for the Louvre, \\\n",
        "which holds the largest collection in the world of da Vinci’s paintings, as well as 22 drawings.'\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Leonardo da Vinci exhibition is held under the high patronage of French President Emmanuel Macron. The year 2019 marks the 500-year anniversary of the death of Leonardo da Vinci in France, of particular importance for the Louvre, which holds the largest collection in the world of da Vinci’s paintings, as well as 22 drawings.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP6Q2NyzKNpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528a627f-d265-4d39-bed9-55a65ff33da2"
      },
      "source": [
        "# передаем текст и создаем SpaCy Document объект\n",
        "doc = nlp(text)\n",
        "type(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbWs4VrdMQRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea24298-9503-47b7-a472-4b61555f815c"
      },
      "source": [
        "# полный перечень методов класса https://spacy.io/api/doc\n",
        "print(*dir(doc), sep ='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_\n",
            "__bytes__\n",
            "__class__\n",
            "__delattr__\n",
            "__dir__\n",
            "__doc__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattribute__\n",
            "__getitem__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__iter__\n",
            "__le__\n",
            "__len__\n",
            "__lt__\n",
            "__ne__\n",
            "__new__\n",
            "__pyx_vtable__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__setstate__\n",
            "__sizeof__\n",
            "__str__\n",
            "__subclasshook__\n",
            "__unicode__\n",
            "_bulk_merge\n",
            "_get_array_attrs\n",
            "_py_tokens\n",
            "_realloc\n",
            "_vector\n",
            "_vector_norm\n",
            "cats\n",
            "char_span\n",
            "copy\n",
            "count_by\n",
            "doc\n",
            "ents\n",
            "extend_tensor\n",
            "from_array\n",
            "from_bytes\n",
            "from_dict\n",
            "from_disk\n",
            "from_docs\n",
            "get_extension\n",
            "get_lca_matrix\n",
            "has_annotation\n",
            "has_extension\n",
            "has_unknown_spaces\n",
            "has_vector\n",
            "is_nered\n",
            "is_parsed\n",
            "is_sentenced\n",
            "is_tagged\n",
            "lang\n",
            "lang_\n",
            "mem\n",
            "noun_chunks\n",
            "noun_chunks_iterator\n",
            "remove_extension\n",
            "retokenize\n",
            "sentiment\n",
            "sents\n",
            "set_ents\n",
            "set_extension\n",
            "similarity\n",
            "spans\n",
            "tensor\n",
            "text\n",
            "text_with_ws\n",
            "to_array\n",
            "to_bytes\n",
            "to_dict\n",
            "to_disk\n",
            "to_json\n",
            "to_utf8_array\n",
            "user_data\n",
            "user_hooks\n",
            "user_span_hooks\n",
            "user_token_hooks\n",
            "vector\n",
            "vector_norm\n",
            "vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoqN8PU8M9uS"
      },
      "source": [
        "В данном задании будет использован только метод **sents**.\n",
        "\n",
        "(Из документации Spacy):  \n",
        "sents: YIELDS\tSentences in the document. \n",
        "\n",
        "Т.е. это генератор, каждый итерируемый элемент которого - предложение (результат применения sentencizer).  \n",
        "Тип элементов: spacy.tokens.span.Span, методы этого класса пересекаются с методами spacy.tokens.doc.Doc.\n",
        "\n",
        "\\* _Возвращаемые генератором значения можно преобразовать в список._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgzf2ZSGMTov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee22edd-fa48-43db-a3f3-3e806dcb5507"
      },
      "source": [
        "doc.sents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator at 0x7fcbe920d730>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYk4JYANnUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b7a71a-a6c4-401c-9f5f-d40b5f3b0fa8"
      },
      "source": [
        "for sent in doc.sents:\n",
        "  print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Leonardo da Vinci exhibition is held under the high patronage of French President Emmanuel Macron.\n",
            "The year 2019 marks the 500-year anniversary of the death of Leonardo da Vinci in France, of particular importance for the Louvre, which holds the largest collection in the world of da Vinci’s paintings, as well as 22 drawings.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW46uM9R-hrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e340d9c-92fe-45eb-f59b-a566b34eeaa1"
      },
      "source": [
        "print(*list(doc.sents), sep='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Leonardo da Vinci exhibition is held under the high patronage of French President Emmanuel Macron.\n",
            "\n",
            "The year 2019 marks the 500-year anniversary of the death of Leonardo da Vinci in France, of particular importance for the Louvre, which holds the largest collection in the world of da Vinci’s paintings, as well as 22 drawings.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF5QDrOKNsmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc9a631-d845-41e8-d77c-7aa169e33d07"
      },
      "source": [
        "for sent in doc.sents:\n",
        "  print(type(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.span.Span'>\n",
            "<class 'spacy.tokens.span.Span'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhI40iAsN9uo"
      },
      "source": [
        "###Span and Token attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-mEFNRfOESY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3bea9f-6ecc-49e1-9aef-b22bfe2b9479"
      },
      "source": [
        "dir(spacy.tokens.span.Span)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '_fix_dep_copy',\n",
              " '_vector',\n",
              " '_vector_norm',\n",
              " 'as_doc',\n",
              " 'char_span',\n",
              " 'conjuncts',\n",
              " 'doc',\n",
              " 'end',\n",
              " 'end_char',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ents',\n",
              " 'get_extension',\n",
              " 'get_lca_matrix',\n",
              " 'has_extension',\n",
              " 'has_vector',\n",
              " 'kb_id',\n",
              " 'kb_id_',\n",
              " 'label',\n",
              " 'label_',\n",
              " 'lefts',\n",
              " 'lemma_',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'noun_chunks',\n",
              " 'orth_',\n",
              " 'remove_extension',\n",
              " 'rights',\n",
              " 'root',\n",
              " 'sent',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'similarity',\n",
              " 'start',\n",
              " 'start_char',\n",
              " 'subtree',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'to_array',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_2xiIaUNyvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a65ba15-e74b-4e14-a297-2cd0299079f8"
      },
      "source": [
        "# Основные атрибуты\n",
        "for sent in doc.sents:\n",
        "  print('Sentence:', sent)\n",
        "  print('Length:',sent.end-sent.start) # Число слов (токенов) в предложении (учитыв. и знаки препинания)\n",
        "  print('Noun chunks:',set(sent.noun_chunks)) # base noun phrases\n",
        "  print('Ents:',sent.ents) # named entities in the span\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: The Leonardo da Vinci exhibition is held under the high patronage of French President Emmanuel Macron.\n",
            "Length: 17\n",
            "Noun chunks: {the high patronage, French President Emmanuel Macron, The Leonardo da Vinci exhibition}\n",
            "Ents: [The Leonardo da Vinci, French, Emmanuel Macron]\n",
            "\n",
            "Sentence: The year 2019 marks the 500-year anniversary of the death of Leonardo da Vinci in France, of particular importance for the Louvre, which holds the largest collection in the world of da Vinci’s paintings, as well as 22 drawings.\n",
            "Length: 46\n",
            "Noun chunks: {the 500-year anniversary, France, Leonardo da Vinci, particular importance, da Vinci’s paintings, the Louvre, the largest collection, 22 drawings, the world, The year, the death}\n",
            "Ents: [The year 2019 marks, 500-year, Leonardo da Vinci, France, 22]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-drtwXBlUm0i"
      },
      "source": [
        "Noun chunks - именные группы.  \n",
        "Ents = named entity - список слов/фраз предложения, которые были распознаны в результате [Named entity recognition](https://monkeylearn.com/blog/named-entity-recognition/).  \n",
        "Посмотрим на типы named entities в тексте.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86KOEVPBUmn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7034b2ff-ba99-4acb-de9a-b8c210b1f630"
      },
      "source": [
        "for sent in doc.sents:\n",
        "  entities = zip(sent.ents, [(s.label_, spacy.explain(str(s.label_))) for s in sent.ents])\n",
        "  print('named entities:',*entities, sep ='\\n', end = '\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "named entities:\n",
            "(The Leonardo da Vinci, ('PERSON', 'People, including fictional'))\n",
            "(French, ('NORP', 'Nationalities or religious or political groups'))\n",
            "(Emmanuel Macron, ('PERSON', 'People, including fictional'))\n",
            "\n",
            "named entities:\n",
            "(The year 2019 marks, ('DATE', 'Absolute or relative dates or periods'))\n",
            "(500-year, ('DATE', 'Absolute or relative dates or periods'))\n",
            "(Leonardo da Vinci, ('PERSON', 'People, including fictional'))\n",
            "(France, ('GPE', 'Countries, cities, states'))\n",
            "(22, ('CARDINAL', 'Numerals that do not fall under another type'))\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgZOwAogZFCF"
      },
      "source": [
        "Полный список named entities:\n",
        "\n",
        "\"PERSON\": \"People, including fictional\",  \n",
        "\"NORP\": \"Nationalities or religious or political groups\",  \n",
        "\"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",  \n",
        "\"FAC\": \"Buildings, airports, highways, bridges, etc.\",  \n",
        "\"ORG\": \"Companies, agencies, institutions, etc.\",  \n",
        "\"GPE\": \"Countries, cities, states\",  \n",
        "\"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",  \n",
        "\"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",  \n",
        "\"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n",
        "\"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n",
        "\"LAW\": \"Named documents made into laws.\",  \n",
        "\"LANGUAGE\": \"Any named language\",  \n",
        "\"DATE\": \"Absolute or relative dates or periods\",  \n",
        "\"TIME\": \"Times smaller than a day\",  \n",
        "\"PERCENT\": 'Percentage, including \"%\"',  \n",
        "\"MONEY\": \"Monetary values, including unit\",  \n",
        "\"QUANTITY\": \"Measurements, as of weight or distance\",  \n",
        "\"ORDINAL\": '\"first\", \"second\", etc.',  \n",
        "\"CARDINAL\": \"Numerals that do not fall under another type\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6ZEvzaBJdV"
      },
      "source": [
        "При итерировании по предложению возвращаемые объекты являются экземплярами класса **Token**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r1Qn8w-UmiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35a0162-0ac0-45ae-ca93-20dd04e1e152"
      },
      "source": [
        "for token in list(doc.sents)[0]:\n",
        "    print(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The\n",
            "Leonardo\n",
            "da\n",
            "Vinci\n",
            "exhibition\n",
            "is\n",
            "held\n",
            "under\n",
            "the\n",
            "high\n",
            "patronage\n",
            "of\n",
            "French\n",
            "President\n",
            "Emmanuel\n",
            "Macron\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSz_1d-iCDO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c505e627-f1e1-4903-e6cb-de3917e72ef2"
      },
      "source": [
        "print(type(list(doc.sents)[0][0]))\n",
        "print(*dir(list(doc.sents)[0][0]), sep ='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n",
            "_\n",
            "__bytes__\n",
            "__class__\n",
            "__delattr__\n",
            "__dir__\n",
            "__doc__\n",
            "__eq__\n",
            "__format__\n",
            "__ge__\n",
            "__getattribute__\n",
            "__gt__\n",
            "__hash__\n",
            "__init__\n",
            "__init_subclass__\n",
            "__le__\n",
            "__len__\n",
            "__lt__\n",
            "__ne__\n",
            "__new__\n",
            "__pyx_vtable__\n",
            "__reduce__\n",
            "__reduce_ex__\n",
            "__repr__\n",
            "__setattr__\n",
            "__sizeof__\n",
            "__str__\n",
            "__subclasshook__\n",
            "__unicode__\n",
            "ancestors\n",
            "check_flag\n",
            "children\n",
            "cluster\n",
            "conjuncts\n",
            "dep\n",
            "dep_\n",
            "doc\n",
            "ent_id\n",
            "ent_id_\n",
            "ent_iob\n",
            "ent_iob_\n",
            "ent_kb_id\n",
            "ent_kb_id_\n",
            "ent_type\n",
            "ent_type_\n",
            "get_extension\n",
            "has_dep\n",
            "has_extension\n",
            "has_head\n",
            "has_morph\n",
            "has_vector\n",
            "head\n",
            "i\n",
            "idx\n",
            "iob_strings\n",
            "is_alpha\n",
            "is_ancestor\n",
            "is_ascii\n",
            "is_bracket\n",
            "is_currency\n",
            "is_digit\n",
            "is_left_punct\n",
            "is_lower\n",
            "is_oov\n",
            "is_punct\n",
            "is_quote\n",
            "is_right_punct\n",
            "is_sent_end\n",
            "is_sent_start\n",
            "is_space\n",
            "is_stop\n",
            "is_title\n",
            "is_upper\n",
            "lang\n",
            "lang_\n",
            "left_edge\n",
            "lefts\n",
            "lemma\n",
            "lemma_\n",
            "lex\n",
            "lex_id\n",
            "like_email\n",
            "like_num\n",
            "like_url\n",
            "lower\n",
            "lower_\n",
            "morph\n",
            "n_lefts\n",
            "n_rights\n",
            "nbor\n",
            "norm\n",
            "norm_\n",
            "orth\n",
            "orth_\n",
            "pos\n",
            "pos_\n",
            "prefix\n",
            "prefix_\n",
            "prob\n",
            "rank\n",
            "remove_extension\n",
            "right_edge\n",
            "rights\n",
            "sent\n",
            "sent_start\n",
            "sentiment\n",
            "set_extension\n",
            "set_morph\n",
            "shape\n",
            "shape_\n",
            "similarity\n",
            "subtree\n",
            "suffix\n",
            "suffix_\n",
            "tag\n",
            "tag_\n",
            "tensor\n",
            "text\n",
            "text_with_ws\n",
            "vector\n",
            "vector_norm\n",
            "vocab\n",
            "whitespace_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtGjoMjESoLr"
      },
      "source": [
        "Т.е. с помощью атрибутов выше можно:  \n",
        "\n",
        "получить список связанных с токеном слов с п. союзов (conjuncts), текст токена (text), префикс, суффикс, частеречный тег (POS tag (part-of-speech tag) и TAG (The detailed part-of-speech tag )), тег зависимости (DEP (dependency tag)) и тд..  \n",
        "\n",
        "Некоторые атрибуты _Token_ аналогичны, но имеют 2 вариации: с подчеркиванием и без, при использовании вариации с подчеркиванием будет возвращено значение типа string, без - ID значение int.\n",
        "Сфокусируемся на атрибутах, которые пригодятся при выполнении задания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMNaKyKfDjLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c32744-d4d9-41b2-cb73-d83d39bcd4e0"
      },
      "source": [
        "print(f\"{'Token' : <13}{'.i' : <5}{'.head' : <10}{'.norm_' : <13}\\\n",
        "    {'.lemma_' : <13}{'.pos_' : <10}{'.tag_ ': <10}{'.dep_': <10}\") \n",
        "for token in list(doc.sents)[0]:\n",
        "    #print(*[token, token.text, token.lemma_, token.tag_, token.pos_, token.dep_], sep = '\\t')\n",
        "    print(f\"{str(token) : <13}{str(token.i) : <5}{str(token.head) : <10}{str(token.norm_) : <13}\\\n",
        "    {str(token.lemma_) : <13}{str(token.pos_) : <10}{str(token.tag_ ): <10}{str(token.dep_): <10}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token        .i   .head     .norm_           .lemma_      .pos_     .tag_     .dep_     \n",
            "The          0    exhibitionthe              the          DET       DT        det       \n",
            "Leonardo     1    Vinci     leonardo         Leonardo     PROPN     NNP       compound  \n",
            "da           2    Vinci     da               da           PROPN     NNP       compound  \n",
            "Vinci        3    exhibitionvinci            Vinci        PROPN     NNP       compound  \n",
            "exhibition   4    held      exhibition       exhibition   NOUN      NN        nsubjpass \n",
            "is           5    held      is               be           AUX       VBZ       auxpass   \n",
            "held         6    held      held             hold         VERB      VBN       ROOT      \n",
            "under        7    held      under            under        ADP       IN        prep      \n",
            "the          8    patronage the              the          DET       DT        det       \n",
            "high         9    patronage high             high         ADJ       JJ        amod      \n",
            "patronage    10   under     patronage        patronage    NOUN      NN        pobj      \n",
            "of           11   patronage of               of           ADP       IN        prep      \n",
            "French       12   President french           french       ADJ       JJ        amod      \n",
            "President    13   Macron    president        President    PROPN     NNP       compound  \n",
            "Emmanuel     14   Macron    emmanuel         Emmanuel     PROPN     NNP       compound  \n",
            "Macron       15   of        macron           Macron       PROPN     NNP       pobj      \n",
            ".            16   held      .                .            PUNCT     .         punct     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5zRel86IxfQ"
      },
      "source": [
        "Список всех тегов можно посмотреть [здесь](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py). \n",
        "\n",
        "Для удобства словарь тегов для english language models представлен внизу."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-lRTL6EJOlW"
      },
      "source": [
        "GLOSSARY = {\n",
        "    # POS tags\n",
        "    # Universal POS Tags\n",
        "    # http://universaldependencies.org/u/pos/\n",
        "    \"ADJ\": \"adjective\",\n",
        "    \"ADP\": \"adposition\",\n",
        "    \"ADV\": \"adverb\",\n",
        "    \"AUX\": \"auxiliary\",\n",
        "    \"CONJ\": \"conjunction\",\n",
        "    \"CCONJ\": \"coordinating conjunction\",\n",
        "    \"DET\": \"determiner\",\n",
        "    \"INTJ\": \"interjection\",\n",
        "    \"NOUN\": \"noun\",\n",
        "    \"NUM\": \"numeral\",\n",
        "    \"PART\": \"particle\",\n",
        "    \"PRON\": \"pronoun\",\n",
        "    \"PROPN\": \"proper noun\",\n",
        "    \"PUNCT\": \"punctuation\",\n",
        "    \"SCONJ\": \"subordinating conjunction\",\n",
        "    \"SYM\": \"symbol\",\n",
        "    \"VERB\": \"verb\",\n",
        "    \"X\": \"other\",\n",
        "    \"EOL\": \"end of line\",\n",
        "    \"SPACE\": \"space\",\n",
        "    # POS tags (English)\n",
        "    # OntoNotes 5 / Penn Treebank\n",
        "    # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "    # https://universaldependencies.org/docs/en/pos/\n",
        "    \".\": \"punctuation mark, sentence closer\",\n",
        "    \",\": \"punctuation mark, comma\",\n",
        "    \"-LRB-\": \"left round bracket\",\n",
        "    \"-RRB-\": \"right round bracket\",\n",
        "    \"``\": \"opening quotation mark\",\n",
        "    '\"\"': \"closing quotation mark\",\n",
        "    \"''\": \"closing quotation mark\",\n",
        "    \":\": \"punctuation mark, colon or ellipsis\",\n",
        "    \"$\": \"symbol, currency\",\n",
        "    \"#\": \"symbol, number sign\",\n",
        "    \"AFX\": \"affix\",\n",
        "    \"CC\": \"conjunction, coordinating\",\n",
        "    \"CD\": \"cardinal number\",\n",
        "    \"DT\": \"determiner\",\n",
        "    \"EX\": \"existential there\",\n",
        "    \"FW\": \"foreign word\",\n",
        "    \"HYPH\": \"punctuation mark, hyphen\",\n",
        "    \"IN\": \"conjunction, subordinating or preposition\",\n",
        "    \"JJ\": \"adjective\",\n",
        "    \"JJR\": \"adjective, comparative\",\n",
        "    \"JJS\": \"adjective, superlative\",\n",
        "    \"LS\": \"list item marker\",\n",
        "    \"MD\": \"verb, modal auxiliary\",\n",
        "    \"NIL\": \"missing tag\",\n",
        "    \"NN\": \"noun, singular or mass\",\n",
        "    \"NNP\": \"noun, proper singular\",\n",
        "    \"NNPS\": \"noun, proper plural\",\n",
        "    \"NNS\": \"noun, plural\",\n",
        "    \"PDT\": \"predeterminer\",\n",
        "    \"POS\": \"possessive ending\",\n",
        "    \"PRP\": \"pronoun, personal\",\n",
        "    \"PRP$\": \"pronoun, possessive\",\n",
        "    \"RB\": \"adverb\",\n",
        "    \"RBR\": \"adverb, comparative\",\n",
        "    \"RBS\": \"adverb, superlative\",\n",
        "    \"RP\": \"adverb, particle\",\n",
        "    \"TO\": 'infinitival \"to\"',\n",
        "    \"UH\": \"interjection\",\n",
        "    \"VB\": \"verb, base form\",\n",
        "    \"VBD\": \"verb, past tense\",\n",
        "    \"VBG\": \"verb, gerund or present participle\",\n",
        "    \"VBN\": \"verb, past participle\",\n",
        "    \"VBP\": \"verb, non-3rd person singular present\",\n",
        "    \"VBZ\": \"verb, 3rd person singular present\",\n",
        "    \"WDT\": \"wh-determiner\",\n",
        "    \"WP\": \"wh-pronoun, personal\",\n",
        "    \"WP$\": \"wh-pronoun, possessive\",\n",
        "    \"WRB\": \"wh-adverb\",\n",
        "    \"SP\": \"space\",\n",
        "    \"ADD\": \"email\",\n",
        "    \"NFP\": \"superfluous punctuation\",\n",
        "    \"GW\": \"additional word in multi-word expression\",\n",
        "    \"XX\": \"unknown\",\n",
        "    \"BES\": 'auxiliary \"be\"',\n",
        "    \"HVS\": 'forms of \"have\"',\n",
        "    # Noun chunks\n",
        "    \"NP\": \"noun phrase\",\n",
        "    \"PP\": \"prepositional phrase\",\n",
        "    \"VP\": \"verb phrase\",\n",
        "    \"ADVP\": \"adverb phrase\",\n",
        "    \"ADJP\": \"adjective phrase\",\n",
        "    \"SBAR\": \"subordinating conjunction\",\n",
        "    \"PRT\": \"particle\",\n",
        "    \"PNP\": \"prepositional noun phrase\",\n",
        "    # Dependency Labels (English)\n",
        "    # ClearNLP / Universal Dependencies\n",
        "    # https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
        "    # https://universaldependencies.org/docs/en/dep/\n",
        "     \"acl\": \"clausal modifier of noun (adjectival clause)\",\n",
        "    \"acomp\": \"adjectival complement\",\n",
        "    \"advcl\": \"adverbial clause modifier\",\n",
        "    \"advmod\": \"adverbial modifier\",\n",
        "    \"agent\": \"agent\",\n",
        "    \"amod\": \"adjectival modifier\",\n",
        "    \"appos\": \"appositional modifier\",\n",
        "    \"attr\": \"attribute\",\n",
        "    \"aux\": \"auxiliary\",\n",
        "    \"auxpass\": \"auxiliary (passive)\",\n",
        "    \"case\": \"case marking\",\n",
        "    \"cc\": \"coordinating conjunction\",\n",
        "    \"ccomp\": \"clausal complement\",\n",
        "    \"clf\": \"classifier\",\n",
        "    \"complm\": \"complementizer\",\n",
        "    \"compound\": \"compound\",\n",
        "    \"conj\": \"conjunct\",\n",
        "    \"cop\": \"copula\",\n",
        "    \"csubj\": \"clausal subject\",\n",
        "    \"csubjpass\": \"clausal subject (passive)\",\n",
        "    \"dative\": \"dative\",\n",
        "    \"dep\": \"unclassified dependent\",\n",
        "    \"det\": \"determiner\",\n",
        "    \"discourse\": \"discourse element\",\n",
        "    \"dislocated\": \"dislocated elements\",\n",
        "    \"dobj\": \"direct object\",\n",
        "    \"expl\": \"expletive\",\n",
        "    \"fixed\": \"fixed multiword expression\",\n",
        "    \"flat\": \"flat multiword expression\",\n",
        "    \"goeswith\": \"goes with\",\n",
        "    \"hmod\": \"modifier in hyphenation\",\n",
        "    \"hyph\": \"hyphen\",\n",
        "    \"infmod\": \"infinitival modifier\",\n",
        "    \"intj\": \"interjection\",\n",
        "    \"iobj\": \"indirect object\",\n",
        "    \"list\": \"list\",\n",
        "    \"mark\": \"marker\",\n",
        "    \"meta\": \"meta modifier\",\n",
        "    \"neg\": \"negation modifier\",\n",
        "    \"nmod\": \"modifier of nominal\",\n",
        "    \"nn\": \"noun compound modifier\",\n",
        "    \"npadvmod\": \"noun phrase as adverbial modifier\",\n",
        "    \"nsubj\": \"nominal subject\",\n",
        "    \"nsubjpass\": \"nominal subject (passive)\",\n",
        "    \"nounmod\": \"modifier of nominal\",\n",
        "    \"npmod\": \"noun phrase as adverbial modifier\",\n",
        "    \"num\": \"number modifier\",\n",
        "    \"number\": \"number compound modifier\",\n",
        "    \"nummod\": \"numeric modifier\",\n",
        "    \"oprd\": \"object predicate\",\n",
        "    \"obj\": \"object\",\n",
        "    \"obl\": \"oblique nominal\",\n",
        "    \"orphan\": \"orphan\",\n",
        "    \"parataxis\": \"parataxis\",\n",
        "    \"partmod\": \"participal modifier\",\n",
        "    \"pcomp\": \"complement of preposition\",\n",
        "    \"pobj\": \"object of preposition\",\n",
        "    \"poss\": \"possession modifier\",\n",
        "    \"possessive\": \"possessive modifier\",\n",
        "    \"preconj\": \"pre-correlative conjunction\",\n",
        "    \"prep\": \"prepositional modifier\",\n",
        "    \"prt\": \"particle\",\n",
        "    \"punct\": \"punctuation\",\n",
        "    \"quantmod\": \"modifier of quantifier\",\n",
        "    \"rcmod\": \"relative clause modifier\",\n",
        "    \"relcl\": \"relative clause modifier\",\n",
        "    \"reparandum\": \"overridden disfluency\",\n",
        "    \"root\": \"root\",\n",
        "    \"vocative\": \"vocative\",\n",
        "    \"xcomp\": \"open clausal complement\",    # Named Entity Recognition\n",
        "    # OntoNotes 5 (Список был раньше)\n",
        "    # https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf\n",
        "    \"PERSON\": \"People, including fictional\",\n",
        "    \"NORP\": \"Nationalities or religious or political groups\",\n",
        "    \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n",
        "    \"FAC\": \"Buildings, airports, highways, bridges, etc.\",\n",
        "    \"ORG\": \"Companies, agencies, institutions, etc.\",\n",
        "    \"GPE\": \"Countries, cities, states\",\n",
        "    \"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",\n",
        "    \"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",\n",
        "    \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n",
        "    \"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n",
        "    \"LAW\": \"Named documents made into laws.\",\n",
        "    \"LANGUAGE\": \"Any named language\",\n",
        "    \"DATE\": \"Absolute or relative dates or periods\",\n",
        "    \"TIME\": \"Times smaller than a day\",\n",
        "    \"PERCENT\": 'Percentage, including \"%\"',\n",
        "    \"MONEY\": \"Monetary values, including unit\",\n",
        "    \"QUANTITY\": \"Measurements, as of weight or distance\",\n",
        "    \"ORDINAL\": '\"first\", \"second\", etc.',\n",
        "    \"CARDINAL\": \"Numerals that do not fall under another type\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-os3dICqMxOK"
      },
      "source": [
        "[back_to_homework](#scrollTo=bOE9xktYHVex&line=2&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDUDnETPJ0bo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "83bff0a5-2fe2-4e6e-9bdc-5524188c799f"
      },
      "source": [
        "GLOSSARY[\"nsubj\"] # Примеры предложений: https://universaldependencies.org/docs/en/dep/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'nominal subject'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoHvrahtMfj1"
      },
      "source": [
        "### Token.morph\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb0fZQtaMenc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194450c6-3c9b-49ae-d2af-376808f32119"
      },
      "source": [
        "print(f\"{'Token' : <12}{'.morph' : <12}\") \n",
        "for token in list(doc.sents)[0]:\n",
        "    print(f\"{str(token) : <12}{str(token.morph) : <12}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token       .morph      \n",
            "The         Definite=Def|PronType=Art\n",
            "Leonardo    NounType=Prop|Number=Sing\n",
            "da          NounType=Prop|Number=Sing\n",
            "Vinci       NounType=Prop|Number=Sing\n",
            "exhibition  Number=Sing \n",
            "is          Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
            "held        Aspect=Perf|Tense=Past|VerbForm=Part\n",
            "under                   \n",
            "the         Definite=Def|PronType=Art\n",
            "high        Degree=Pos  \n",
            "patronage   Number=Sing \n",
            "of                      \n",
            "French      Degree=Pos  \n",
            "President   NounType=Prop|Number=Sing\n",
            "Emmanuel    NounType=Prop|Number=Sing\n",
            "Macron      NounType=Prop|Number=Sing\n",
            ".           PunctType=Peri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6zsNYxnOk9z"
      },
      "source": [
        "При выполнении домашнего задания этот атрибут можно не использовать.  \n",
        "О возвращаемых значениях этого атрибута: [morph_format](https://universaldependencies.org/format.html#morphological-annotation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2vZdkgXLiBz"
      },
      "source": [
        "# Matcher patterns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fqmGa5UPB35"
      },
      "source": [
        "Чтобы проверить, подходит ли предложение под задаваемый паттерн, можно пользоваться одним из трех классов SpaCy:\n",
        "\n",
        "1) **Matcher** - для построения паттерна можно использовать pos и tag теги и атрибуты token.\n",
        "\n",
        "2) **DependencyMatcher** - можно использовать синтаксические зависимости (be a sibling of) и создать паттерн синтаксического дерева.\n",
        "\n",
        "3) **PhraseMatcher** - match конкретных фраз и словочетаний.\n",
        "\n",
        "## Matcher  \n",
        "Сначала нужно составить pattern, затем создать ```matcher = Matcher(vocab=nlp.vocab)``` передать pattern в matcher, ```matcher.add(\"название паттерна\", patterns=[pattern])```. \n",
        "\n",
        "Результаты текстовых совпадений для каждого doc можно посмотреть так: ```matcher(doc, as_spans=True)```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlYnSZKWT4zM"
      },
      "source": [
        "Создание паттерна - основная задача.  \n",
        "В случае **Matcher** pattern обязан быть списком  _list_ словарей _dict_. Каждый словарь соответствует одному токену, ключи - атрибуты токена, также можно использовать ключ _OP_. Возможные значения _OP_:\n",
        "\n",
        "1.   '!'\tNegate the pattern, by requiring it to match exactly 0 times.\n",
        "2. '?'\tMake the pattern optional, by allowing it to match 0 or 1 times.\n",
        "3. '+'\tRequire the pattern to match 1 or more times.\n",
        "4. '*'\tAllow the pattern to match 0 or more times.\n",
        "\n",
        "\n",
        "Другие доступные ключи и их описание: [Pattern format](https://spacy.io/api/matcher)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fab9sV6BLhNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb51df1-2349-4d93-b62c-8a325d35e961"
      },
      "source": [
        "matcher = Matcher(vocab=nlp.vocab)\n",
        "matcher"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.matcher.matcher.Matcher at 0x7fcbe7f3d200>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KymOXhGZQbZy"
      },
      "source": [
        "noun_phrase_verb = [{'POS': 'NOUN', 'OP': '+'}, {'POS': 'VERB', 'OP': '+'}]\n",
        "matcher.add(\"noun_phrase+verb\", patterns=[noun_phrase_verb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZvQMZfCdcyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590aa763-a730-45fb-da67-d2ae66d65fc1"
      },
      "source": [
        "# загрузка input.txt = realec_data\n",
        "!wget -O input.txt 'https://drive.google.com/uc?export=download&id=1cK8FATtEGxC2kWv3FCthdrZZWM01c2sw'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-09 15:21:11--  https://drive.google.com/uc?export=download&id=1cK8FATtEGxC2kWv3FCthdrZZWM01c2sw\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.218.139, 172.217.218.100, 172.217.218.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.218.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-8c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9j4oro54u70vqpahuk6cvm6vm5fq5ge1/1620573600000/01108725925562890305/*/1cK8FATtEGxC2kWv3FCthdrZZWM01c2sw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-05-09 15:21:13--  https://doc-10-8c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9j4oro54u70vqpahuk6cvm6vm5fq5ge1/1620573600000/01108725925562890305/*/1cK8FATtEGxC2kWv3FCthdrZZWM01c2sw?e=download\n",
            "Resolving doc-10-8c-docs.googleusercontent.com (doc-10-8c-docs.googleusercontent.com)... 74.125.143.132, 2a00:1450:4013:c03::84\n",
            "Connecting to doc-10-8c-docs.googleusercontent.com (doc-10-8c-docs.googleusercontent.com)|74.125.143.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt               [ <=>                ]   2.05M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-09 15:21:14 (124 MB/s) - ‘input.txt’ saved [2146822]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O03XUunNYF1e"
      },
      "source": [
        "# или \n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jne0ygvfMrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e50557-c9c6-4355-f08a-0f1261712d68"
      },
      "source": [
        "# check path\n",
        "with open('./' + 'input.txt', encoding='utf-8') as file:\n",
        "  for text in file.readlines()[:2]:\n",
        "      doc = nlp(text)\n",
        "      for sent in list(doc.sents)[:2]:\n",
        "          d = nlp(str(sent))\n",
        "          # as_spans=True - для текстового отображения результатов\n",
        "          results = matcher(doc, as_spans=True)\n",
        "          for result in results:\n",
        "            print(nlp.vocab[result.label].text, '\\t', result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "noun_phrase+verb \t complainant wants\n",
            "noun_phrase+verb \t defendant is\n",
            "noun_phrase+verb \t woman named\n",
            "noun_phrase+verb \t judge acquit\n",
            "noun_phrase+verb \t prosecutor revealed\n",
            "noun_phrase+verb \t complainant wants\n",
            "noun_phrase+verb \t defendant is\n",
            "noun_phrase+verb \t woman named\n",
            "noun_phrase+verb \t judge acquit\n",
            "noun_phrase+verb \t prosecutor revealed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg5ecWVnhzuc"
      },
      "source": [
        "Найденные совпадения соответствуют паттерну.  \n",
        "Но, например, при поиске существительного и глагола, между которыми будут слова других частей речи - паттерн будет слишком усложнен. Чтобы решать такие задачи или, например, задачу поиска подлежащего и сказуемого, необходимо использовать синтаксические деревья, т.е., искать такое существительное, родителем которого является root (в случае простого предложения). Для решения таких задач следует использовать **DependencyMatcher**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1dkzFwNH9QY"
      },
      "source": [
        "## DependencyMatcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAewXNvMclF"
      },
      "source": [
        "[back_to_homework](#scrollTo=bOE9xktYHVex&line=2&uniqifier=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbutumZlmxKk"
      },
      "source": [
        "Принцип создания **dep_matcher** аналогичен **Matcher**, но существенно отличается построением паттерна. \n",
        "\n",
        "Описание ключей паттерна можно посмотреть в документации: [Pattern format](https://spacy.io/api/dependencymatcher).\n",
        "\n",
        "_Паттерн для **DependencyMatcher**_\n",
        "\n",
        "Паттерн должен представлять собой список _list_, каждый элемент которого задает match токена и является словарем _dict_. \n",
        "___\n",
        "_Словарь первого элемента_\n",
        "___\n",
        "Словарь первого элемента обязан включать следующие ключи: \n",
        "- `RIGHT_ID`: название токена (задается любым); \n",
        "- `RIGHT_ATTRS`: словарь атрибутов токена.  \n",
        "\n",
        "Атрибуты могут включать любые атрибуты токена.\n",
        "\n",
        "___\n",
        "_Словари остальных элементов_\n",
        "___\n",
        "Словари для matching других токенов должны включать следующих 4 ключа:  `LEFT_ID`, `REL_OP`, `RIGHT_ID`, `RIGHT_ATTRS`.  \n",
        "\n",
        "- `LEFT_ID` : название левой зависимой вершины;\n",
        "- `REL_OP` : операнд, описываемый зависимость токена данного словаря от вершины `LEFT_ID`;\n",
        "\n",
        "- `RIGHT_ID` : название токена (также задается на усмотрение создателя паттерна);\n",
        "- `RIGHT_ATTRS` : словарь, ключами которого являются атрибуты токена.\n",
        "\n",
        "Возможные `REL_OP` [Pattern format](https://spacy.io/api/dependencymatcher):\n",
        "\n",
        "\n",
        "*   A < B:\tA is the immediate dependent of B.\n",
        "* A > B:\tA is the immediate head of B.\n",
        "* A << B:\tA is the dependent in a chain to B following dep → head paths.\n",
        "* A >> B:\tA is the head in a chain to B following head → dep paths.\n",
        "* A . B:\tA immediately precedes B, i.e. A.i == B.i - 1, and both are within the same dependency tree.\n",
        "* A .* B:\tA precedes B, i.e. A.i < B.i, and both are within the same dependency tree (not in Semgrex).\n",
        "* A ; B:\tA immediately follows B, i.e. A.i == B.i + 1, and both are within the same dependency tree (not in Semgrex).\n",
        "* A ;* B:\tA follows B, i.e. A.i > B.i, and both are within the same dependency tree (not in Semgrex).\n",
        "* A \\$+ B:\tB is a right immediate sibling of A, i.e. A and B have the same parent and A.i == B.i - 1.\n",
        "* A \\$- B:\tB is a left immediate sibling of A, i.e. A and B have the same parent and A.i == B.i + 1.\n",
        "* A \\$++ B:\tB is a right sibling of A, i.e. A and B have the same parent and A.i < B.i.\n",
        "* A \\$-- B:\tB is a left sibling of A, i.e. A and B have the same parent and A.i > B.i.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1otwHUfMhID"
      },
      "source": [
        "[back_to_homework](#scrollTo=bOE9xktYHVex&line=2&uniqifier=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3buf6gnYhY4"
      },
      "source": [
        "Для выбора операнда стоит предварительно посмотреть на значения атрибутов token.head, token.root в конкретном предложении."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeZAKFAvmw30"
      },
      "source": [
        "text = '' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB3iuw1XuY4-"
      },
      "source": [
        "# check path\n",
        "with open('./' + 'input.txt', encoding='utf-8') as file:\n",
        "  text = file.readlines()[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szV2BAY6tvFv"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0QwF-QtneH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c83424e-8cfb-4fa9-c1fd-b52e344c9b6b"
      },
      "source": [
        "print(f\"{'Token' : <13}{'.i' : <5}{'.head' : <10}{'.norm_' : <13}\\\n",
        "{'.pos_' : <10}{'.tag_ ': <10}{'.dep_': <10}\") \n",
        "for token in list(doc.sents)[0]:\n",
        "    #print(*[token, token.text, token.lemma_, token.tag_, token.pos_, token.dep_], sep = '\\t')\n",
        "    print(f\"{str(token) : <13}{str(token.i) : <5}{str(token.head) : <10}{str(token.norm_) : <13}\\\n",
        "    {str(token.lemma_) : <13}{str(token.pos_) : <10}{str(token.tag_ ): <10}{str(token.dep_): <10}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token        .i   .head     .norm_       .pos_     .tag_     .dep_     \n",
            "﻿When        0    have      ﻿when            ﻿when        ADP       IN        advmod    \n",
            "a            1    few       a                a            DET       DT        quantmod  \n",
            "few          2    people    few              few          ADJ       JJ        nummod    \n",
            "completely   3    different completely       completely   ADV       RB        advmod    \n",
            "different    4    people    different        different    ADJ       JJ        amod      \n",
            "people       5    have      people           people       NOUN      NNS       nsubj     \n",
            "have         6    have      have             have         VERB      VBP       ROOT      \n",
            "to           7    share     to               to           PART      TO        aux       \n",
            "share        8    have      share            share        VERB      VB        xcomp     \n",
            "a            9    number    a                a            DET       DT        det       \n",
            "flat         10   number    flat             flat         ADJ       JJ        amod      \n",
            ",            11   flat      ,                ,            PUNCT     ,         punct     \n",
            "very         12   often     very             very         ADV       RB        advmod    \n",
            "often        13   flat      often            often        ADV       RB        advmod    \n",
            "a            14   number    a                a            DET       DT        det       \n",
            "number       15   share     number           number       NOUN      NN        dobj      \n",
            "of           16   number    of               of           ADP       IN        prep      \n",
            "difficulties 17   of        difficulties     difficulty   NOUN      NNS       pobj      \n",
            "emerge       18   number    emerge           emerge       VERB      VBP       acl       \n",
            ".            19   have      .                .            PUNCT     .         punct     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAWWUUodvxNf"
      },
      "source": [
        "Попробуем составить паттерн для поиска: number(s) of + существительное.\n",
        "\n",
        "В предложении выше number является head для of, of является head для difficulties.\n",
        "\n",
        "Операнд, соответствующий be a head of: A '<' B.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivy6G2tX2EKY"
      },
      "source": [
        "dep_matcher = DependencyMatcher(vocab=nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llTQjE_R2MzO"
      },
      "source": [
        "dep_pattern = [{'RIGHT_ID': 'number', 'RIGHT_ATTRS': {'NORM': 'number'}},\n",
        "               {'LEFT_ID': 'number', 'REL_OP': '>', 'RIGHT_ID': 'prep', 'RIGHT_ATTRS': {'NORM': 'of'}},\n",
        "                {'LEFT_ID': 'prep', 'REL_OP': '>', 'RIGHT_ID': 'noun', 'RIGHT_ATTRS': {'TAG':{'IN': ['NNS', 'NN']}, 'DEP': 'pobj'}}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBj7vxQVMuIz"
      },
      "source": [
        "[back_to_homework](#scrollTo=bOE9xktYHVex&line=2&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ACxgSXv5vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c39ac5-535a-4fcc-a905-2a296e555462"
      },
      "source": [
        "dep_matcher.add('number_of_pattern', patterns=[dep_pattern])\n",
        "dep_matches = dep_matcher(doc)\n",
        "dep_matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(8480868639249277778, [15, 16, 17])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA9Lkguo4xt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a303937a-8916-4d1d-e42d-e8cbe362f7d6"
      },
      "source": [
        "for match in dep_matches:\n",
        "    pattern_name = match[0]\n",
        "    matches = match[1]\n",
        "    print(nlp.vocab[pattern_name].text, '\\t', doc[ matches[0]], '...', doc[matches[1]], doc[matches[2]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number_of_pattern \t number ... of difficulties\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdURAnRZL88j"
      },
      "source": [
        "[back_to_homework](#scrollTo=ztPdmpqzJypF&line=1&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb--4IQDd4M1"
      },
      "source": [
        "def test(name, pattern, file_name):\n",
        "    data = pd.DataFrame(columns = ['sentence','match'])\n",
        "    with open('./' + file_name, encoding='utf-8') as file:\n",
        "        dep_matcher = DependencyMatcher(vocab=nlp.vocab)\n",
        "        dep_matcher.add(name, patterns=[pattern])\n",
        "        for text in file.readlines(): # [::10]:  шаг 10, чтобы ускорить выполнение ячейки\n",
        "            doc = nlp(text)\n",
        "            for sent in doc.sents:\n",
        "                d = nlp(str(sent))\n",
        "                dep_matches_ = dep_matcher(d)\n",
        "                for match in dep_matches_:\n",
        "                    i_ = data.shape[0]\n",
        "                    data.loc[i_, 'sentence'] = str(sent)\n",
        "                    words = [str(d[i]) for i in match[1]]\n",
        "                    data.loc[i_, 'match'] = ' '.join(words)\n",
        "    return data\n",
        "\n",
        "#df = test('number_of_pattern', dep_pattern, 'input.txt')     \n",
        "#df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdFMmQ6i7auF"
      },
      "source": [
        "Чтобы выполнить задание, нужно понять, какого типа ошибки должна находить функция, затем - придумать предложение с ошибкой и попробовать написать паттерн, используя который DepMatcher отыскивал бы ошибочные интервалы в предложениях с этой ошибкой.\n",
        "\n",
        "Далее стоит проверить, действительно ли написанный паттерн подходит под предложение.\n",
        "\n",
        "Затем получить результаты, например, для файла input и описать их."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywfmE-5h8nlC"
      },
      "source": [
        "Ниже представлены примеры ошибок некоторых функций, для которых можно составить паттерны в задании:\n",
        "\n",
        "- _consider that_;\n",
        "- _It is obvious/evident comma that_ - не должно быть запятой _(../worth+noticing/saying/mentioning/discussing.. +that)_\n",
        "- _cardinal number + hundreds/thousands/millions.._\n",
        "- ошибочный порядок слов в инверсии, например: _No sooner/Under no crcumstances/Only/Little/Not only/At no time + подлежащее + aux_;\n",
        "- ошибки отсутствия запятой после любых вводных слов _Firstly/Secondly/Thirdly.._\n",
        "- _I wish I will_ и аналогичные;\n",
        "- Лишняя инверсия после слов _whether/wh-words/if..: I wonder whether did he do it._.\n",
        "\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyYoh4YrB-SM"
      },
      "source": [
        "# Homework\n",
        "\n",
        "## Types of errors\n",
        "\n",
        "Выберите 1-2 ошибки из списка выше (или другие ошибки из models.py adwiser).\n",
        "\n",
        "Опишите выбранный тип ошибок в ячейке ниже:\n",
        "- В чем заключается ошибка?\n",
        "- Приведите примеры предложений с ошибкой.\n",
        "- Исправьте ошибки в примерах.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A22wqsOzEJiq"
      },
      "source": [
        "___\n",
        "Ответ: \n",
        "\n",
        "1) Лишняя инверсия после слов(а) whether (wh-words, if..)\n",
        "\n",
        "2) I wonder whether **did he do** it. This depends whether **is he** pleased.\n",
        "\n",
        "3) I wonder whether **he did** it. This depends whether **he is** pleased.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXsf4rM7EUva"
      },
      "source": [
        "## Watch tags\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnOkWOoeE-ui"
      },
      "source": [
        "text = 'I wonder whether did he do it. This depends whether is he pleased. I still have to decide whether I want to do this. Why is it so great? Behavioral addictions are patterns of behavior, which are reproduced repeatedly and compulsively even if said action causes serious negative consequences to the persons physical, mental, social, and/or financial well-being.' # Предложения"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42sM0tNyFCj7"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIyMf_Z-FS6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b8977d-34f2-4eb0-d43a-90a23b106c6d"
      },
      "source": [
        "print(f\"{'Token' : <13}{'.i' : <5}{'.head' : <10}{'.norm_' : <13}\\\n",
        "{'.lemma_' : <13}{'.pos_' : <10}{'.tag_ ': <10}{'.dep_': <10}\") \n",
        "for sent in list(doc.sents):\n",
        "  for token in sent:\n",
        "    print(f\"{str(token) : <13}{str(token.i) : <5}{str(token.head) : <10}{str(token.norm_) : <13}\\\n",
        "    {str(token.lemma_) : <13}{str(token.pos_): <10}{str(token.tag_): <10}{str(token.dep_): <10}\") \n",
        "    #print(token,token.i,token.head,token.norm_, token.lemma_,token.pos_, token.tag_,token.dep_) \n",
        "    #print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token        .i   .head     .norm_       .lemma_      .pos_     .tag_     .dep_     \n",
            "I            0    wonder    i                I            PRON      PRP       nsubj     \n",
            "wonder       1    wonder    wonder           wonder       VERB      VBP       ROOT      \n",
            "whether      2    do        whether          whether      SCONJ     IN        mark      \n",
            "did          3    do        did              do           AUX       VBD       aux       \n",
            "he           4    do        he               he           PRON      PRP       nsubj     \n",
            "do           5    wonder    do               do           VERB      VB        ccomp     \n",
            "it           6    do        it               it           PRON      PRP       dobj      \n",
            ".            7    wonder    .                .            PUNCT     .         punct     \n",
            "This         8    depends   this             this         DET       DT        nsubj     \n",
            "depends      9    depends   depends          depend       VERB      VBZ       ROOT      \n",
            "whether      10   is        whether          whether      SCONJ     IN        mark      \n",
            "is           11   depends   is               be           AUX       VBZ       ccomp     \n",
            "he           12   pleased   he               he           PRON      PRP       nsubj     \n",
            "pleased      13   is        pleased          pleased      ADJ       JJ        acomp     \n",
            ".            14   depends   .                .            PUNCT     .         punct     \n",
            "I            15   have      i                I            PRON      PRP       nsubj     \n",
            "still        16   have      still            still        ADV       RB        advmod    \n",
            "have         17   have      have             have         VERB      VBP       ROOT      \n",
            "to           18   decide    to               to           PART      TO        aux       \n",
            "decide       19   have      decide           decide       VERB      VB        xcomp     \n",
            "whether      20   want      whether          whether      SCONJ     IN        mark      \n",
            "I            21   want      i                I            PRON      PRP       nsubj     \n",
            "want         22   decide    want             want         VERB      VBP       ccomp     \n",
            "to           23   do        to               to           PART      TO        aux       \n",
            "do           24   want      do               do           VERB      VB        xcomp     \n",
            "this         25   do        this             this         DET       DT        dobj      \n",
            ".            26   have      .                .            PUNCT     .         punct     \n",
            "Why          27   is        why              why          ADV       WRB       advmod    \n",
            "is           28   is        is               be           AUX       VBZ       ROOT      \n",
            "it           29   is        it               it           PRON      PRP       nsubj     \n",
            "so           30   great     so               so           ADV       RB        advmod    \n",
            "great        31   is        great            great        ADJ       JJ        acomp     \n",
            "?            32   is        ?                ?            PUNCT     .         punct     \n",
            "Behavioral   33   addictionsbehavioral       behavioral   ADJ       JJ        amod      \n",
            "addictions   34   are       addictions       addiction    NOUN      NNS       nsubj     \n",
            "are          35   are       are              be           AUX       VBP       ROOT      \n",
            "patterns     36   are       patterns         pattern      NOUN      NNS       attr      \n",
            "of           37   patterns  of               of           ADP       IN        prep      \n",
            "behavior     38   of        behavior         behavior     NOUN      NN        pobj      \n",
            ",            39   patterns  ,                ,            PUNCT     ,         punct     \n",
            "which        40   reproducedwhich            which        DET       WDT       nsubjpass \n",
            "are          41   reproducedare              be           AUX       VBP       auxpass   \n",
            "reproduced   42   patterns  reproduced       reproduce    VERB      VBN       relcl     \n",
            "repeatedly   43   reproducedrepeatedly       repeatedly   ADV       RB        advmod    \n",
            "and          44   repeatedlyand              and          CCONJ     CC        cc        \n",
            "compulsively 45   repeatedlycompulsively     compulsively ADV       RB        conj      \n",
            "even         46   said      even             even         ADV       RB        advmod    \n",
            "if           47   said      if               if           SCONJ     IN        mark      \n",
            "said         48   are       said             say          VERB      VBD       advcl     \n",
            "action       49   causes    action           action       NOUN      NN        nsubj     \n",
            "causes       50   said      causes           cause        VERB      VBZ       ccomp     \n",
            "serious      51   consequencesserious          serious      ADJ       JJ        amod      \n",
            "negative     52   consequencesnegative         negative     ADJ       JJ        amod      \n",
            "consequences 53   causes    consequences     consequence  NOUN      NNS       dobj      \n",
            "to           54   consequencesto               to           ADP       IN        prep      \n",
            "the          55   persons   the              the          DET       DT        det       \n",
            "persons      56   to        persons          person       NOUN      NNS       pobj      \n",
            "physical     57   persons   physical         physical     ADJ       JJ        amod      \n",
            ",            58   physical  ,                ,            PUNCT     ,         punct     \n",
            "mental       59   physical  mental           mental       ADJ       JJ        conj      \n",
            ",            60   mental    ,                ,            PUNCT     ,         punct     \n",
            "social       61   mental    social           social       ADJ       JJ        conj      \n",
            ",            62   social    ,                ,            PUNCT     ,         punct     \n",
            "and/or       63   social    and/or           and/or       CCONJ     CC        cc        \n",
            "financial    64   social    financial        financial    ADJ       JJ        conj      \n",
            "well         65   being     well             well         NOUN      NN        compound  \n",
            "-            66   being     -                -            PUNCT     HYPH      punct     \n",
            "being        67   persons   being            being        NOUN      NN        appos     \n",
            ".            68   are       .                .            PUNCT     .         punct     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxdKu3RWFqA5"
      },
      "source": [
        "Как соотносятся (pos, tag, dep) слова, подходящие под паттерн ошибки?\n",
        "\n",
        "Опишите теги, которые можно будет использовать для построения паттернов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO5j5ExjF-p7"
      },
      "source": [
        "___\n",
        "Ответ: A .* B\n",
        "\n",
        "dep_pattern = [\n",
        "  \n",
        "  {'RIGHT_ID': 'whether', 'RIGHT_ATTRS': {'NORM': 'whether'}},\n",
        "  \n",
        "  {'LEFT_ID': 'whether', 'REL_OP': '.', 'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'TAG':{'IN': ['VBP', 'VBZ', 'VBD']}}},\n",
        "  \n",
        "  {'LEFT_ID': 'verb', 'REL_OP': '.', 'RIGHT_ID': 'noun', 'RIGHT_ATTRS': {'DEP': 'nsubj'}}\n",
        "  \n",
        "  ]\n",
        "___\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXkVMdlSGDVg"
      },
      "source": [
        "## Patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHQUpe_yIimu"
      },
      "source": [
        "[DependencyMatcher](#scrollTo=q1dkzFwNH9QY&line=1&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOE9xktYHVex"
      },
      "source": [
        "dep_matcher_ = DependencyMatcher(vocab=nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGhMblADIwZW"
      },
      "source": [
        "[Pattern](#scrollTo=llTQjE_R2MzO&line=2&uniqifier=1)\n",
        "[Glossary](#scrollTo=O-lRTL6EJOlW&line=2&uniqifier=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz7vgmM_Hcjy"
      },
      "source": [
        "# pattern\n",
        "\"\"\"pattern_one = [\n",
        "                {'RIGHT_ID': 'wh-word', 'RIGHT_ATTRS': {'TAG':{'IN': [\"WDT\", \"WP\", \"WP$\", \"WRB\", 'IN']}}},\n",
        "                {'LEFT_ID': 'wh-word', 'REL_OP': '.', 'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'TAG':{'IN': [\"VB\", \"VBD\", \"VBP\", \"VBZ\"]}}},\n",
        "                {'LEFT_ID': 'verb', 'REL_OP': '.', 'RIGHT_ID': 'noun', 'RIGHT_ATTRS': {'DEP': 'nsubj'}},\n",
        "                {'LEFT_ID': 'noun', 'REL_OP': '.*', 'RIGHT_ID': 'punct', 'RIGHT_ATTRS': {'NORM': {'IN': ['.', '!']}}}\n",
        "                ]\"\"\" # too many correct sentences counted as incorrect\n",
        "#pattern_two = []\n",
        "dep_pattern_ = [\n",
        "                {'RIGHT_ID': 'wh-word', 'RIGHT_ATTRS': {'NORM': {'IN': ['whether', 'if']}}},\n",
        "                {'LEFT_ID': 'wh-word', 'REL_OP': '.', 'RIGHT_ID': 'verb', 'RIGHT_ATTRS': {'TAG':{'IN': [\"VB\", \"VBD\", \"VBP\", \"VBZ\"]}}},\n",
        "                {'LEFT_ID': 'verb', 'REL_OP': '.', 'RIGHT_ID': 'noun', 'RIGHT_ATTRS': {'DEP': 'nsubj'}}\n",
        "                ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGfI-NecGVLj"
      },
      "source": [
        "Посмотрите, находится ли составленный паттерн в предложениях"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NENpZPdgJV_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce66262-61f4-442b-9c31-bada1f812361"
      },
      "source": [
        "dep_matcher_.add('whether_inversion', patterns=[dep_pattern_]) \n",
        "dep_matches = dep_matcher_(doc)\n",
        "dep_matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4432688271610773680, [2, 3, 4]),\n",
              " (4432688271610773680, [10, 11, 12]),\n",
              " (4432688271610773680, [47, 48, 49])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUf6v7MVJYqG"
      },
      "source": [
        "for match in dep_matches:\n",
        "    pattern_name = match[0]\n",
        "    matches = match[1]\n",
        "    # Количество эл-ов matches зависит от cardinality составленного паттерна\n",
        "    # print(nlp.vocab[pattern_name].text, '\\t', doc[ matches[0]], '...', doc[matches[1]], doc[matches[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in_8vQYzKE3C"
      },
      "source": [
        "## Test on Realec data or your own input\n",
        "\n",
        "Посмотрите, какие совпадения были найдены в input или в составленном Вами файле."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vZV1D_VKQc9"
      },
      "source": [
        "В Colab загрузить свой файл с предложениями txt можно, например, кликнув знак папки (Files) в меню слева или через google.colab upload."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsEu-JYELsHr"
      },
      "source": [
        "Можно пользоваться функцией [test](#scrollTo=Vb--4IQDd4M1&line=1&uniqifier=1). (в функции `test` шаг 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztPdmpqzJypF"
      },
      "source": [
        "# Поиск паттернов может занять продолжительное время (~10-15 минут)\n",
        "# Чтобы раскомментировать код: ctrl+/\n",
        "# df = ...\n",
        "# df_two = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmShovPDO_kr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "426c4e24-eb74-44f1-8225-638735a2d563"
      },
      "source": [
        "df_my = test('whether_inversion', dep_pattern_, 'input.txt')\n",
        "df_my"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Behavioral addictions are patterns of behavior...</td>\n",
              "      <td>if said action</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>After reading this book I was puzzled whether ...</td>\n",
              "      <td>whether is it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence           match\n",
              "0  Behavioral addictions are patterns of behavior...  if said action\n",
              "1  After reading this book I was puzzled whether ...   whether is it"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiLcFL4MWQXN",
        "outputId": "35e51146-2ae7-402b-bd07-6759f42aa004"
      },
      "source": [
        "for n, sen in enumerate(df_my['sentence']):\n",
        "  print(n)\n",
        "  print(sen)\n",
        "  print(df_my['match'][n])\n",
        "  print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Behavioral addictions are patterns of behavior, which are reproduced repeatedly and compulsively even if said action causes serious negative consequences to the person's physical, mental, social, and/or financial well-being.\n",
            "if said action\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "After reading this book I was puzzled whether is it a real story and if it is who inspired Sparks ?\n",
            "whether is it\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ1TmzR3M9FL"
      },
      "source": [
        "## Save the results and analyze them\n",
        "\n",
        "Были ли пойманы matcher-ом предложения без рассматриваемых ошибок?\n",
        "\n",
        "С чем это связано, можно ли внести изменения в pattern(s)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnwo--F9RoU2"
      },
      "source": [
        "___\n",
        "Ответ:\n",
        "\n",
        "1) Паттерн, который установлен сейчас, находит лишнее предложение с if, потому что причастие said рассматривается как глагол (хотя я удалила теги \"VBG\", \"VBN\" из поиска)\n",
        "\n",
        "2) Паттерн, который закоменитрован в той же ячейке, где и рабочий паттерн, находит слишком много правильных пердложений, так как я пыталась найти как можно больше wh-слов, но находятся конструкции, где инверсия нужна\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q2lbHy7MLac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47ed9df-e6b7-4a24-fa5b-0e579d0d023e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-53cUxRASJVO"
      },
      "source": [
        "%mkdir results_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCoLui-_SmeW"
      },
      "source": [
        "df_my.to_excel('results_table/pattern_1.xlsx') #.to_csv('results_table/pattern_1.csv')\n",
        "# df_two.to_excel('drive/results_table/pattern_2.xlsx') #..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}